{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import SimpleITK as sitk # to read nii files\n",
    "from livelossplot import PlotLossesKerasTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv3D = partial(keras.layers.Conv3D, kernel_size=3, strides=(1,)*3,\n",
    "        padding=\"SAME\", use_bias=True)\n",
    "\n",
    "class ResidualUnit(keras.layers.Layer):\n",
    "    # separate construction and execution\n",
    "    # be aware of the strides' shape\n",
    "    def __init__(self, filters, strides=(1,)*3, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "                \n",
    "        # a list a layers that can be iterated\n",
    "        self.main_layers = [\n",
    "                DefaultConv3D(self.filters, strides=self.strides, kernel_initializer=\"he_normal\"),\n",
    "                self.activation,\n",
    "                DefaultConv3D(self.filters, strides=(1,)*3, kernel_initializer=\"he_normal\"),\n",
    "                ]\n",
    "        self.skip_layers = []\n",
    "        if np.prod(self.strides) > 1:\n",
    "            #self.skip_layers = [keras.layers.MaxPool3D(pool_size=(2,)*3, strides=strides, padding=\"SAME\")]\n",
    "            \n",
    "            self.skip_layers = [\n",
    "                DefaultConv3D(self.filters, kernel_size=1, strides=self.strides, kernel_initializer=\"he_normal\")\n",
    "                ]          \n",
    "            \n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        orig_x = inputs\n",
    "        \n",
    "        for layer in self.main_layers:\n",
    "            x = layer(x) # f(x)\n",
    "        \n",
    "        for layer in self.skip_layers:\n",
    "            orig_x = layer(orig_x)\n",
    "        \n",
    "        return self.activation(x + orig_x)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ResidualUnit, self).get_config()\n",
    "        config.update({'filters': self.filters, 'strides':self.strides})\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model(\"./my_logs/Bi_rotate_aug_Classes_epoch371_best.h5\", custom_objects={\"ResidualUnit\": ResidualUnit})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def backend_reshape(x, shape):\n",
    "#    return keras.backend.reshape(x, shape)\n",
    "filters = (16, 32, 64, 128)\n",
    "strides = (1, 2, 2, 2)\n",
    "new_model = keras.models.Sequential()\n",
    "\n",
    "'''\n",
    "for idx, layer in enumerate(model.layers):\n",
    "    if idx != len(model.layers) - 1:\n",
    "        new_model.add(layer)'''\n",
    "new_model.add(keras.layers.Lambda(lambda x: keras.backend.reshape(x, shape = (-1,48, 96, 96, 1) )))\n",
    "new_model.add(DefaultConv3D(filters[0], kernel_size=3, strides=(1,)*3,\n",
    "        input_shape=[48, 96, 96, 1], kernel_initializer=\"he_normal\"))\n",
    "new_model.add(keras.layers.Activation(\"relu\"))\n",
    "#new_model.add(keras.layers.MaxPool3D(pool_size=(2,)*3, padding=\"SAME\"))\n",
    "\n",
    "for filter, stride in zip(filters[1:], strides[1:]):\n",
    "    new_model.add(ResidualUnit(filter, strides=(stride,)*3))\n",
    "    new_model.add(ResidualUnit(filter, strides=(1,)*3))\n",
    "\n",
    "new_model.add(keras.layers.GlobalAvgPool3D())\n",
    "new_model.add(keras.layers.Flatten()) # 64\n",
    "\n",
    "#new_model.add(keras.layers.Dropout(0.3))\n",
    "new_model.add(keras.layers.Lambda(lambda x: keras.backend.reshape(x, shape = (1, -1, 128))))\n",
    "new_model.add(keras.layers.LSTM(256, return_sequences= True))\n",
    "new_model.add(keras.layers.Dropout(0.3))\n",
    "new_model.add(keras.layers.LSTM(128, return_sequences= True))\n",
    "new_model.add(keras.layers.Dropout(0.3))\n",
    "new_model.add(keras.layers.Lambda(lambda x: x[:,-1,:]))\n",
    "new_model.add(keras.layers.Dense(2, activation = \"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\")\n",
    "new_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.zeros(shape = (1, 4, 48, 96, 96, 1), dtype = np.float32)\n",
    "\n",
    "output = new_model(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = pd.read_csv(\"New_CRNN_file_reference.csv\")\n",
    "file_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"../Data\"\n",
    "\n",
    "file_ls = [ ]\n",
    "label_ls = [ ]\n",
    "group_ls = [ ]\n",
    "for _, row in file_df.iterrows():\n",
    "    files, labels, groups = row[[\"File\", \"Label\", \"Group\"]]\n",
    "    files = files[1:-1].replace(\"'\", \"\").split(\", \")\n",
    "    labels = np.array(labels, dtype = np.int).reshape(1,1)\n",
    "    groups = np.array(groups, dtype = np.int).reshape(1,1)\n",
    "    file_ls.append(files)\n",
    "    label_ls.append(labels)\n",
    "    group_ls.append(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_img, test_img, train_l, test_l = train_test_split(\n",
    "        file_ls, label_ls, test_size=0.3, random_state=48\n",
    ")\n",
    "#del imgs\n",
    "# split test data into validation and evaluation evenly\n",
    "val_img, evl_img, val_l, evl_l = train_test_split(\n",
    "        test_img, test_l, test_size = 0.5, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_percent(y_ls):\n",
    "    p_count = 0\n",
    "    for y in y_ls:\n",
    "        if y.sum() != 0:\n",
    "            p_count += 1\n",
    "    return p_count , len(y_ls)\n",
    "\n",
    "print(perf_percent(label_ls))\n",
    "print(perf_percent(train_l))\n",
    "print(perf_percent(val_l))\n",
    "print(perf_percent(evl_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(img):\n",
    "    if type(img) == sitk.Image:\n",
    "        img = sitk.GetArrayFromImage(img)\n",
    "    mean = np.mean(img)\n",
    "    std = np.std(img)\n",
    "    img = (img - mean) / std\n",
    "    img = img[:,:,:,np.newaxis].astype(np.float32)\n",
    "    return img\n",
    "\n",
    "def RotateAugmentation(img_ls):\n",
    "    '''\n",
    "    Description: rotate the input 3D image along z-axis by random angles \n",
    "    Args:\n",
    "        img: sitk.Image, dimension is 3\n",
    "    Return: sitk.Image\n",
    "    '''\n",
    "    dim = img_ls[0].GetSize()\n",
    "    translation = [np.random.randint(0, 5), np.random.randint(0, 5), 0]\n",
    "    angle = np.float64(np.random.randint(0, 360) / 180 * np.pi)\n",
    "    rotation = [np.float64(0), np.float64(0), angle] # x-axis, y-axis, z-axis angles\n",
    "    center = [(d-1) // 2 for d in dim]\n",
    "    transformation = sitk.Euler3DTransform()    \n",
    "    transformation.SetCenter(center)            # SetCenter takes three args\n",
    "    transformation.SetRotation(*rotation)         # SetRotation takes three args\n",
    "    transformation.SetTranslation(translation)\n",
    "    aug_ls = []\n",
    "    for img in img_ls:\n",
    "        img.SetOrigin((0,0,0))\n",
    "        img.SetSpacing((1.0, 1.0, 1.0))\n",
    "\n",
    "        aug_img = sitk.Resample(sitk.Cast(img, sitk.sitkFloat32), \n",
    "                sitk.Cast(img[0:96, 0:96, :], \n",
    "                sitk.sitkFloat32), transformation, \n",
    "                sitk.sitkLinear, 0.0, sitk.sitkFloat32\n",
    "                )\n",
    "        aug_ls.append(aug_img)\n",
    "    return aug_ls\n",
    "\n",
    "def IdentityAugmentation(img_ls):\n",
    "    return [img[2:98, 2:98, :] for img in img_ls]\n",
    "\n",
    "def FlipAugmentation(img_ls):\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataGenerator(file_ls, label_ls, augmentation = None, example_n = 6):\n",
    "    def generator():\n",
    "        for fs, ys in zip(file_ls, label_ls):\n",
    "            img_ls = [sitk.ReadImage(os.path.join(data_path, f)) for f in fs]\n",
    "            if augmentation is not None:\n",
    "                for i in range(example_n):\n",
    "                    aug_ls = augmentation(img_ls)\n",
    "                    yield [normalization(img) for img in aug_ls], ys\n",
    "                yield [normalization(img[2:98, 2:98]) for img in img_ls], ys\n",
    "            else:\n",
    "                yield [normalization(img[2:98, 2:98]) for img in img_ls], ys\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DatasetReader(file_ls, label_ls, shuffle_size, batch_size, augmentation = None, example_n = 6):\n",
    "        generator = DataGenerator(file_ls, label_ls, augmentation = augmentation, example_n = example_n)\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_types = (tf.float32, tf.float32),\n",
    "            output_shapes = (\n",
    "                tf.TensorShape((None, 48, 96, 96, 1)), \n",
    "                tf.TensorShape((None,1))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        dataset = dataset.repeat().shuffle(shuffle_size).batch(batch_size)\n",
    "        \n",
    "        return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "SHUBFFLE_SIZE = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DatasetReader(train_img, train_l, \n",
    "        shuffle_size = SHUBFFLE_SIZE, \n",
    "        batch_size = BATCH_SIZE, augmentation = RotateAugmentation, example_n = 3\n",
    ")\n",
    "val_set = DatasetReader(val_img, val_l, 40, BATCH_SIZE) #, augmentation = IdentityAugmentation, example_n = 1, special_multiplier = 1\n",
    "evl_set = DatasetReader(evl_img, evl_l, 40, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for l in train_set.take(5):\n",
    "    print(l[0].shape, l[1].shape)\n",
    "    y_test_pred = new_model(l[0])\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(l[1], y_test_pred)\n",
    "    print(loss)\n",
    "    print(new_model.train_on_batch(l[0], l[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== Configure Callbacks ==================\n",
    "#_{epoch}\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(r\"./my_logs/Bidirectional_Bi_C-RNN_{epoch}_no_batch_norm.h5\", \n",
    "        monitor = 'val_accuracy', mode = 'max',\n",
    "        save_best_only=True\n",
    "        )\n",
    "\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f} \\n\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir(comment=\"_no_batch_norm\"):\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S{}\".format(comment))\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "\n",
    "plotlosses = PlotLossesKerasTF()\n",
    "callbacks = [checkpoint_cb, tensorboard_cb, plotlosses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = new_model.fit(train_set, steps_per_epoch = 440, epochs = 400,\n",
    "                       validation_data = val_set,\n",
    "                        initial_epoch = 0,\n",
    "                       validation_steps = 66, \n",
    "                        callbacks=callbacks,\n",
    "                        verbose = 2\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights(\"./my_logs/Bidirectional_Bi_C-RNN/Bidirectional_Bi_C-RNN_166_no_batch_norm.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.evaluate(evl_set, steps = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ls = []\n",
    "for bidx, fs in enumerate(file_ls):\n",
    "    x = np.zeros(shape = (1, 5, 48, 96, 96, 1))\n",
    "    xls = []\n",
    "    for sidx, imgf in enumerate(fs):\n",
    "        img = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(data_path,imgf)))\n",
    "        img = normalization(img[:, 2:98, 2:98])\n",
    "        xls.append(img)\n",
    "        \n",
    "    x_ls.append(np.array(xls)[np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "wrong_count = 0\n",
    "pred_bi = []\n",
    "pred_y = []\n",
    "for x, y in zip(x_ls, label_ls):\n",
    "    x = np.array(x)\n",
    "    x = x[np.newaxis, :, :]\n",
    "    pred = new_model.predict_classes(np.array(x))\n",
    "    print(\"Predicted is : {}    ; True label is: {}    ;\".format(np.squeeze(pred), np.array(y)))\n",
    "    print()\n",
    "    a = all(np.squeeze(pred) == np.array(y))\n",
    "    pred_bi.append(a)\n",
    "    pred_y.append(np.squeeze(pred))\n",
    "    if a:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        wrong_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct_count)\n",
    "print(wrong_count)\n",
    "print(correct_count / (wrong_count + correct_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_points = []\n",
    "for y in label_ls:\n",
    "    y = np.array(y)\n",
    "    trans = 1\n",
    "    for idx, j in enumerate(y):\n",
    "        if j == 0:\n",
    "            trans += 1\n",
    "        elif j == 1:\n",
    "            continue\n",
    "            \n",
    "        if idx == (y.shape[0] - 1) and j == 0:\n",
    "            trans = 'Infinity'\n",
    "            \n",
    "    transit_points.append(str(trans))\n",
    "    print(y, trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame()\n",
    "acc_df['Transition_point'] = transit_points\n",
    "acc_df['y_True'] = label_ls\n",
    "acc_df['y_pred'] = pred_y\n",
    "acc_df['T/F'] = pred_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df[['Transition_point','T/F']].groupby('Transition_point').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
